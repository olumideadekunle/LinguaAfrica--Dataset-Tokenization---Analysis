{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olumideadekunle/LinguaAfrica--Dataset-Tokenization---Analysis/blob/main/LinguaAfrica_Dataset_Tokenization_%26_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6kkanWKTSzW4"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Olumide Adekunle B.\n",
        "\n",
        "##Google DeepMind Project"
      ],
      "metadata": {
        "id": "Armqpc2YzWiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ],
      "metadata": {
        "id": "3myzQnLMOJ91"
      }
    },
    {
      "metadata": {
        "id": "keR5bAwlrIo4"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab: Prepare The Dataset For Training an SLM\n",
        "\n",
        "\n",
        "Preparing the dataset so that a transformer model can process it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "This lab guides you through preparing a text dataset for training a small language model (SLM). The SLM that you will train is a transformer model. Transformer models and, more generally, neural network models, require data to be in a specific format so that they can process them. Specifically, when processing texts, you first have to tokenize the text into tokens. Then, you have to translate these tokens into unique numeric IDs before you can process them with a transformer model.\n",
        "\n",
        "In this lab, you will focus on these necessary **pre-processing steps** such as tokenization, vocabulary creation, and mapping tokens to their IDs. This will build the foundation for training your SLM in the next lab.\n",
        "\n"
      ],
      "metadata": {
        "id": "8WbdzNNC7Jrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will know:\n",
        "* The data format requirements of transformer models and how to map tokens to token IDs and vice versa.\n",
        "* How to prepare a dataset for training a transformer model."
      ],
      "metadata": {
        "id": "XYhCTN9FNLwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "As in previous labs, you will load the Africa Galore dataset and tokenize it using a space tokenizer. You will then build all the ingredients necessary for converting a dataset such that it can be used for training a transformer model.\n",
        "\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset and tokenize it.\n",
        "* Construct a list of all tokens in the dataset.\n",
        "* Construct a list of unique tokens in the dataset.\n",
        "* Create a mapping of tokens to token IDs and a mapping of token IDs to tokens.\n",
        "* Define functions that can translate between tokens and their corresponding IDs.\n",
        "* Define a Python class that encapsulates all methods necessary for preparing the data for a transformer model.\n"
      ],
      "metadata": {
        "id": "iefhLcRUNGPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (‚ñ∂). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ‚åò+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62000feb-c407-4c92-c86b-45295b3c6ceb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today is Monday.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you. In that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime ‚Üí Run before__  from the menu above (or use the keyboard combination Ctrl/‚åò + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will use the [Pandas](http://pandas.pydata.org) package for reading the dataset. Run the following cell to import these packages."
      ],
      "metadata": {
        "id": "UPJE5CKOA2bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import re # Used for splitting strings on spaces.\n",
        "\n",
        "# Packages used.\n",
        "import pandas as pd # For reading the dataset.\n",
        "import textwrap # For adding linebreaks to paragraphs.\n",
        "\n",
        "# For providing feedback.\n",
        "from ai_foundations.feedback.course_1 import slm"
      ],
      "metadata": {
        "id": "m8QsDdu20F0C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aS8lTu4FDclK"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading and tokenizing the dataset\n",
        "\n",
        "As in the previous labs, you will again use the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset for the activities in this lab.\n",
        "\n",
        "Run the following cell to download the dataset and print its first paragraph."
      ]
    },
    {
      "metadata": {
        "id": "x8rRtd8p2DQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643ceeb8-1e8d-48d8-8aa7-3a4e7b0539cd"
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"]\n",
        "print(f\"Loaded Africa Galore dataset with {len(dataset)} paragraphs.\")\n",
        "print(f\"\\nFirst paragraph:\")\n",
        "print(textwrap.fill(dataset[0]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Africa Galore dataset with 232 paragraphs.\n",
            "\n",
            "First paragraph:\n",
            "The Lagos air was thick with humidity, but the energy in the club was\n",
            "electric. The band launched into a hypnotic Afrobeat groove, the drums\n",
            "pounding out a complex polyrhythm, the horns blaring a soaring melody,\n",
            "and the bass laying down a deep, funky foundation. A woman named Imani\n",
            "moved effortlessly to the music, her body swaying in time with the\n",
            "rhythm. The music seemed to flow through her, a powerful current of\n",
            "energy and joy. All around her, people were dancing, singing, and\n",
            "clapping, caught up in the infectious rhythm. The music was more than\n",
            "just entertainment; it was a celebration of life, a connection to\n",
            "their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with n-gram language models, you also have to tokenize sequences before you can use them to train a transformer model. You will again use a space tokenizer that splits sequences on spaces.\n",
        "\n",
        "Run the following cell to define and test the space tokenizer that is implemented by the function `space_tokenize`. This function is almost identical to the function you have already seen. Instead of the string `split` function, it uses the [`re.split`](https://docs.python.org/3/library/re.html#re.split) function, since it's better at handling texts that contain multiple spaces."
      ],
      "metadata": {
        "id": "W93OAFTde5Bg"
      }
    },
    {
      "metadata": {
        "id": "JjaimYtAx4j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54354402-a94d-47fa-810c-01e1d4243a89"
      },
      "cell_type": "code",
      "source": [
        "def space_tokenize(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of tokens.\n",
        "\n",
        "    Splits text on space.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
        "    \"\"\"\n",
        "    # Use `re` package so that splitting on multiple spaces also works.\n",
        "    tokens = re.split(r\" +\", text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Tokenize an example text with the `space_tokenize` function.\n",
        "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kanga,',\n",
              " 'a',\n",
              " 'colorful',\n",
              " 'printed',\n",
              " 'cloth',\n",
              " 'is',\n",
              " 'more',\n",
              " 'than',\n",
              " 'just',\n",
              " 'a',\n",
              " 'fabric.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1: Build a list of all tokens in the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "NnnZKo5MgcO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> üíª **Your task:**\n",
        ">\n",
        "> Complete the following cell to construct a `tokens` list that contains all tokens in the dataset in the order that they appear in.\n",
        ">\n",
        "> You will have to loop through all paragraphs in the dataset and then extract all tokens for each paragraph and add them to the `tokens` list.\n",
        ">\n",
        "> Once you have completed your implementation, run the next two cells to build the list and test your code.\n",
        "------"
      ],
      "metadata": {
        "id": "4FlJiQ0nPscn"
      }
    },
    {
      "metadata": {
        "id": "oqGWDrXokzbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e234af-bac7-46c5-9343-7b52b313de90"
      },
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "\n",
        "# Add your code here.\n",
        "\n",
        "print(f\"Total number of tokens in the Africa Galore dataset: {len(tokens):,}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in the Africa Galore dataset: 0\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "\n",
        "slm.test_build_tokens_list(tokens, space_tokenize, dataset)"
      ],
      "metadata": {
        "id": "6a30c61PJ3Oa",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "20f9b970-056f-4275-cc77-3f3d187b2242"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>‚ùå Sorry, your answer is not correct.</h3><p>`tokens` does not include all tokens from the dataset in the correct order.</p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like a hint? Type Yes or No. Yes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Hint:</h3><p>\n",
              "    1. Use a for loop to go through every paragraph in the dataset.<br>\n",
              "    2. Split each paragraph into individual tokens using the\n",
              "       <code>split_text</code> function.\n",
              "       </p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "_Jt1OWe9tY6n"
      },
      "cell_type": "markdown",
      "source": [
        "To remind yourself what the tokenized dataset looks like, run the following cell to print the first 30 tokens of the first paragraph in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "Pdr8290eEum9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129a1383-0f8c-48cf-a089-09c81dacfcee"
      },
      "cell_type": "code",
      "source": [
        "tokens[:30]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "OWRQLYO7hhp4"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2: Build the vocabulary\n",
        "\n",
        "Transformer models use a fixed set of tokens that they can process and generate. This set of tokens is known as the **vocabulary**. In many cases, this vocabulary is set to the list of unique tokens that appear in the data that the model is being trained on."
      ]
    },
    {
      "metadata": {
        "id": "FbYDEE-Hymhk"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> üíª **Your task:**\n",
        ">\n",
        "> Complete the `build_vocabulary` function that should return the list of\n",
        "> unique tokens that appear in `tokens`.\n",
        ">>\n",
        "> Once you have implemented this function, run the two cells to define the function and test your code.\n",
        "------"
      ]
    },
    {
      "metadata": {
        "id": "ueYSAjGzj2jp"
      },
      "cell_type": "code",
      "source": [
        "def build_vocabulary(tokens: list[str]) -> list[str]:\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocabulary = sorted(list(set(tokens)))\n",
        "    return vocabulary"
      ],
      "outputs": [],
      "execution_count": 20
    },
    {
      "metadata": {
        "id": "q9Fk7rH5kF58",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f329a60a-2393-4f29-c8e9-b93d6f58468f"
      },
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "slm.test_build_vocabulary(build_vocabulary)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nice! Your answer looks correct.\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "2RXLjfFozd9a"
      },
      "cell_type": "markdown",
      "source": [
        "You can now use the function that you have implemented to construct the vocabulary for the Africa Galore dataset.\n",
        "\n",
        "Run the cell below to create the vocabulary and print its size, that is, the number of unique tokens in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "t-mULo_viTXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe57ead-0de2-4fd5-acbb-a92bd405c0f2"
      },
      "cell_type": "code",
      "source": [
        "vocabulary = build_vocabulary(tokens)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(\n",
        "    \"Total number of unique tokens in the Africa Galore dataset:\"\n",
        "    f\" {vocabulary_size:,}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique tokens in the Africa Galore dataset: 0\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "LX9mlGBEzpjz"
      },
      "cell_type": "markdown",
      "source": [
        "To get a sense of what the vocabulary looks like, run the following cell that prints the first 30 entries of the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "YQFWt9KQGN6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae02e22a-225e-42db-a802-41929b9cb397"
      },
      "cell_type": "code",
      "source": [
        "vocabulary[:30]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        " Note that, unlike when you printed the first 30 tokens in the dataset, there are no duplicate entries. Every token appears exactly once in the vocabulary."
      ],
      "metadata": {
        "id": "1OGfWejzSLx9"
      }
    },
    {
      "metadata": {
        "id": "-Z6vtwjGLQcH"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert the tokens into token IDs (indices)\n",
        "\n",
        "As discussed above, in order to train a transformer on a text dataset, you have to turn the text data into a list of **token IDs**. These IDs are numbers such that each token maps uniquely to a different number. The IDs should always be consecutive. That means that, if the vocabulary has size $k$, then each token should map to an ID between 0 and $k-1$.\n",
        "\n",
        "In practice, the translation between tokens and IDs is implemented using two dictionaries:\n",
        "\n",
        "1. **`token_to_index`**: This dictionary maps each token in the vocabulary to its corresponding ID (index). The index must be between 0 and the vocabulary size $k-1$.\n",
        "2. **`index_to_token`**: This dictionary maps an ID (index) back to its corresponding token (a string). Given an index between 0 and $k-1$, it returns the token at that position.\n",
        "\n",
        "When you need to convert a token to a number, use `token_to_index`.\n",
        "And when you need to convert a number back to a token, use `index_to_token`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build `token_to_index`"
      ],
      "metadata": {
        "id": "Yzf768LIWZ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows you how to implement the construction of the `token_to_index` mapping from the vocabulary. If you are not familiar with the [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate) function in Python, print the `index` and `token` on each iteration to get a sense of what it is doing and why this code is creating the correct mapping."
      ],
      "metadata": {
        "id": "g_4HUhihVtxm"
      }
    },
    {
      "metadata": {
        "id": "mP40_BT2VWpK"
      },
      "cell_type": "code",
      "source": [
        "# Build the `token_to_index` dictionary.\n",
        "token_to_index = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "    token_to_index[token] = index"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3: Build `index_to_token`"
      ],
      "metadata": {
        "id": "0ODRXS1UVsNB"
      }
    },
    {
      "metadata": {
        "id": "jmBQMQQXVWpI"
      },
      "cell_type": "markdown",
      "source": [
        "Next, create a dictionary below called `index_to_token`, where the index is the key and the token is the value. This dictionary should be the reverse of the `token_to_index` dictionary. After implementing the dictionary, run the cell and verify that the tokens and their corresponding indexes match between `token_to_index` and `index_to_token`:"
      ]
    },
    {
      "metadata": {
        "id": "HpM5X-RNW89A"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> üíª **Your task:**\n",
        ">\n",
        "> Complete the following cell such that it constructs the `index_to_token` mapping that maps all token IDs to their corresponding strings versions of the tokens.\n",
        ">\n",
        "> **Hint**: It may be useful to iterate through the entries in `token_to_index` using  `token_to_index.items()` to obtain the pairs of indices and tokens.\n",
        ">\n",
        "> Once you have implemented this function, run the two cells to construct the dictionary and test your code.\n",
        "------"
      ]
    },
    {
      "metadata": {
        "id": "rXlsJqD1VWpK"
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps an index (a number) back to\n",
        "# its corresponding token in the vocab.\n",
        "index_to_token = {}\n",
        "\n",
        "# Add your code here."
      ],
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "slm.test_index_to_token(index_to_token, vocabulary)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CvATRQpdVWpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e22df30-cf70-4b08-ad27-a4e2c8c8fcd4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how `token_to_index` and `index_to_token` are inverse mappings, take a look at ten entries of both dictionaries:"
      ],
      "metadata": {
        "id": "d2fFi34dbkBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"token_to_index:\\n\")\n",
        "\n",
        "count = 0\n",
        "first_ten_indices = []\n",
        "for token, token_id in token_to_index.items():\n",
        "    print(f\"'{token}': {token_id}\")\n",
        "    first_ten_indices.append(token_id)\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "        break\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"index_to_token:\\n\")\n",
        "for token_id in first_ten_indices:\n",
        "    print(f\"{token_id}: '{index_to_token[token_id]}'\")"
      ],
      "metadata": {
        "id": "-XLmjxpfbw4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bf9ea2-02cf-44fa-84c0-f9b3a09acab3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_to_index:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "index_to_token:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see above that the first ten tokens all have IDs between zero and ten and these IDs map back to exactly the same ten tokens."
      ],
      "metadata": {
        "id": "Sh-JnvCCcJW7"
      }
    },
    {
      "metadata": {
        "id": "VgOkb5VSwnbm"
      },
      "cell_type": "markdown",
      "source": [
        "## Encode and decode functions\n",
        "\n",
        "Rather than manually translating between lists of tokens and lists of token indices, it can be much easier to convert between these two representations of your data by implementing an `encode` and a `decode` function.\n",
        "\n",
        "- The `encode` function takes a string of text and returns the corresponding indices of the tokens.\n",
        "- The `decode` function takes a list of indices and returns the text associated with it.\n",
        "\n",
        "The following cell provides an implementation of these two functions. Run it to define both of them."
      ]
    },
    {
      "metadata": {
        "id": "JC_dnmc2xjBP"
      },
      "cell_type": "code",
      "source": [
        "def encode(text: str) -> list[int]:\n",
        "    \"\"\"Encodes a text sequence into a list of indices based on the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to be encoded.\n",
        "\n",
        "    Returns:\n",
        "        A list of indices corresponding to the tokens in the input text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert tokens into indices.\n",
        "    indices = []\n",
        "    for token in space_tokenize(text):\n",
        "        token_index = token_to_index.get(token)\n",
        "        indices.append(token_index)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def decode(indices: int | list[int]) -> list[str]:\n",
        "    \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "    Args:\n",
        "        indices: A single index or a list of indices to be decoded into tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: A string of decoded tokens corresponding to the input indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # If a single integer is passed, convert it into a list.\n",
        "    if isinstance(indices, int):\n",
        "        indices = [indices]\n",
        "\n",
        "    # Map indices to tokens.\n",
        "    tokens = []\n",
        "    for index in indices:\n",
        "        token = index_to_token.get(index)\n",
        "        tokens.append(token)\n",
        "\n",
        "    # Join the decoded tokens into a single string.\n",
        "    return \" \".join(tokens)"
      ],
      "outputs": [],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "GXnEuuHPH-Rx"
      },
      "cell_type": "markdown",
      "source": [
        "To verify that these functions are working as expected, you can encode a text so that its tokens are mapped to indices and then decode those indices. The decoding step should return the original text.\n",
        "\n",
        "The following cell prints again the first paragraph in the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "84E5f-ME_0Fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9b9031-7354-4186-bde7-cb182ceb22ea"
      },
      "cell_type": "code",
      "source": [
        "text = dataset[0]\n",
        "print(text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Lagos air was thick with humidity, but the energy in the club was electric. The band launched into a hypnotic Afrobeat groove, the drums pounding out a complex polyrhythm, the horns blaring a soaring melody, and the bass laying down a deep, funky foundation. A woman named Imani moved effortlessly to the music, her body swaying in time with the rhythm. The music seemed to flow through her, a powerful current of energy and joy. All around her, people were dancing, singing, and clapping, caught up in the infectious rhythm. The music was more than just entertainment; it was a celebration of life, a connection to their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "metadata": {
        "id": "7FsCmasazTgc"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following cell to encode the pargraph above and look at the first ten indices."
      ]
    },
    {
      "metadata": {
        "id": "6K2wAHCKHknD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de45727-c38d-4fd0-ebed-054638be485d"
      },
      "cell_type": "code",
      "source": [
        "encode(text)[:10]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "metadata": {
        "id": "nPk5t3kUwpOy"
      },
      "cell_type": "code",
      "source": [
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer that can be initialized with a corpus of texts\n",
        "       or using a provided vocabulary list.\n",
        "\n",
        "    The tokenizer splits the text sequence based on spaces,\n",
        "    using the `encode` method to convert the text into a sequence of indices\n",
        "    and the `decode` method to convert indices back into text.\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        corpus = \"Hello there!\"\n",
        "        tokenizer = SimpleWordTokenizer(corpus)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
        "\n",
        "        Args:\n",
        "            corpus: Input text dataset.\n",
        "            vocabulary: A pre-defined vocabulary. If None,\n",
        "                the vocabulary is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocabulary is None:\n",
        "            # Build the vocabulary from scratch.\n",
        "            if isinstance(corpus, str):\n",
        "                corpus = [corpus]\n",
        "\n",
        "            # Convert text sequence to tokens.\n",
        "            tokens = []\n",
        "            for text in corpus:\n",
        "                for token in self.space_tokenize(text):\n",
        "                    tokens.append(token)\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            self.vocabulary = self.build_vocabulary(tokens)\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
        "        # assigns a unique index to each token.\n",
        "        for index, token in enumerate(self.vocabulary):\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "\n",
        "    def space_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on space into tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split on space.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use re.split such that multiple spaces are treated as a single\n",
        "        # separator.\n",
        "        return re.split(\" +\", text)\n",
        "\n",
        "    def join_text(self, text_list: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string, with tokens separated\n",
        "           by spaces.\n",
        "\n",
        "        Args:\n",
        "            text_list: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined with a space.\n",
        "\n",
        "        \"\"\"\n",
        "        return \" \".join(text_list)\n",
        "\n",
        "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Create a vocabulary list from the list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of tokens in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of unique tokens (vocabulary) in the dataset.\n",
        "        \"\"\"\n",
        "        return sorted(list(set(tokens)))\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            A list of indices corresponding to the tokens in the input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert tokens into indices.\n",
        "        indices = []\n",
        "        for token in self.space_tokenize(text):\n",
        "            token_index = self.token_to_index.get(token)\n",
        "            indices.append(token_index)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "        Args:\n",
        "            indices: A single index or a list of indices to be decoded into\n",
        "                tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(indices, int):\n",
        "            indices = [indices]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = []\n",
        "        for index in indices:\n",
        "            token = self.index_to_token.get(index)\n",
        "            tokens.append(token)\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)"
      ],
      "outputs": [],
      "execution_count": 24
    },
    {
      "metadata": {
        "id": "mgV0fzQB5LrN"
      },
      "cell_type": "markdown",
      "source": [
        "To observe that this class performs the same processing as your previous implementations, run the following cell. This cell runs some tests that verify that the first paragraph from the dataset remains the same after encoding and then decoding it using the `SimpleWordTokenizer`."
      ]
    },
    {
      "metadata": {
        "id": "0rMKmNlRY-3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "5a262b45-edbd-4aca-f6e1-0bd3f67f3e3d"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleWordTokenizer(dataset)\n",
        "slm.test_simple_word_tokenizer(tokenizer, vocabulary, dataset)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>‚ùå Sorry, your answer is not correct.</h3><p>The tokenizer's vocabulary does not match the expected vocabulary.</p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like a hint? Type Yes or No. No\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "metadata": {
        "id": "sZdmYZ0Bqfn9"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the **Prepare The Dataset For Training a SLM** lab.\n",
        "\n",
        "This lab guided you through the steps necessary for preparing a text dataset to be used for training a small language model (SLM). You focused on:\n",
        "\n",
        "- **Loading and exploring the dataset:** You examined the structure and content of the Africa Galore dataset and inspected example paragraphs in the dataset.\n",
        "\n",
        "- **Tokenizing the text:** You used a simple word-level tokenization method to split the text into individual tokens and created a vocabulary of unique tokens.\n",
        "\n",
        "- **Creating numerical representations:** You mapped each token to a unique numerical index by creating `token_to_index` and `index_to_token` dictionaries, which enable the conversion between tokens and token IDs.\n",
        "\n",
        "- **Packaging the steps in a tokenizer class:** You examined a consolidated version of the tokenization and encoding/decoding logic in a reusable `SimpleWordTokenizer` class. This class streamlines the process of converting text into numerical data that can be fed into a language model and converting the output of a language model to human-readable texts.\n",
        "\n",
        "In the next lab, you will use this tokenizer class to tokenize the data and use this data to train a small language model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities above. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "However, we recommend that you *only* look at the solutions after you have tried to solve the activities above *multiple times*. The best way to learn challenging concepts in computer science and artifical intelligence is to debug your code piece-by-piece until it works rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code, for example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also make you practice how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them and then type them manually into the cell. This will help you understand where you went wrong."
      ],
      "metadata": {
        "id": "W9eSg4k_QE9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "kWRihzWaQGnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code to the cell in the coding activity above to build the list of\n",
        "# tokens.\n",
        "for paragraph in dataset:\n",
        "    for token in space_tokenize(paragraph):\n",
        "        tokens.append(token)"
      ],
      "metadata": {
        "id": "GOuSPwbNQKtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "9gQPoK4TkH1q"
      }
    },
    {
      "metadata": {
        "id": "ZeJlbyiBjla4"
      },
      "cell_type": "code",
      "source": [
        "# Complete implementation of the build_vocab function.\n",
        "def build_vocabulary(tokens: list[str])-> list[str]:\n",
        "\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocab = list(set(tokens))\n",
        "    return vocab"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3\n"
      ],
      "metadata": {
        "id": "_dyB98kFkTF9"
      }
    },
    {
      "metadata": {
        "id": "Xan-_ZYxtUpk"
      },
      "cell_type": "code",
      "source": [
        "# Add this code to the cell in the coding activity above to build the\n",
        "# index_to_token mapping.\n",
        "for token, index in token_to_index.items():\n",
        "    index_to_token[index] = token"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59db6334"
      },
      "source": [
        "# Task\n",
        "Complete the tokenization process for the Africa Galore dataset by building a list of all tokens, creating a vocabulary of unique tokens, and establishing mappings between tokens and their numerical indices. Afterward, calculate and visualize the frequencies of the top 20 most frequent tokens in a bar chart, and summarize the tokenization steps and insights gained from the frequency analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d891505"
      },
      "source": [
        "## Complete Coding Activity 1: Build the tokens list\n",
        "\n",
        "### Subtask:\n",
        "Implement the code to populate the `tokens` list by iterating through the `dataset` and applying the `space_tokenize` function to each paragraph. Run the cell (`oqGWDrXokzbj`) to execute your code and then run the test cell (`6a30c61PJ3Oa`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "833d12c9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires populating the `tokens` list by iterating through the `dataset` and using the `space_tokenize` function. I will provide the necessary Python code to achieve this within the specified cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0be11a96",
        "outputId": "1ff3eb5a-4f6f-4c5f-cf43-3ccde3028faa"
      },
      "source": [
        "for paragraph in dataset:\n",
        "    tokens.extend(space_tokenize(paragraph))\n",
        "\n",
        "print(f\"Total number of tokens in the Africa Galore dataset: {len(tokens):,}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens in the Africa Galore dataset: 19,065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba771264"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully populated the `tokens` list. The next instruction is to run the test cell (`6a30c61PJ3Oa`) to verify the implementation of the token list creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78730807",
        "outputId": "967653ea-113c-4f52-eaba-8516c19a3c32"
      },
      "source": [
        "slm.test_build_tokens_list(tokens, space_tokenize, dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d976a653"
      },
      "source": [
        "## Complete Coding Activity 2: Build the vocabulary\n",
        "\n",
        "### Subtask:\n",
        "The `build_vocabulary` function is already defined in cell `ueYSAjGzj2jp`. You need to run this cell to define the function, then run the test cell (`q9Fk7rH5kF58`) to verify it. Finally, run the cell (`t-mULo_viTXK`) to create the `vocabulary` list and print its size.\n",
        "\n",
        "#### Instructions\n",
        "1. Run the code cell `ueYSAjGzj2jp` to define the `build_vocabulary` function.\n",
        "2. Run the test cell `q9Fk7rH5kF58` to verify your implementation.\n",
        "3. Run the code cell `t-mULo_viTXK` to create the `vocabulary` variable and `vocabulary_size`, and print the total number of unique tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02403dab"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions in the previous markdown block, the first step is to run the cell `ueYSAjGzj2jp` which defines the `build_vocabulary` function. This will make the function available for use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a079150"
      },
      "source": [
        "def build_vocabulary(tokens: list[str]):\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocabulary = sorted(list(set(tokens)))\n",
        "    return vocabulary"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c792f31"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions in the previous markdown block, the first step is to run the cell `ueYSAjGzj2jp` which defines the `build_vocabulary` function. This will make the function available for use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06142a1"
      },
      "source": [
        "def build_vocabulary(tokens: list[str]):\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocabulary = sorted(list(set(tokens)))\n",
        "    return vocabulary"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27046e16"
      },
      "source": [
        "**Reasoning**:\n",
        "The `build_vocabulary` function has been defined. Now, following the instructions from the previous markdown block, I need to run the test cell `q9Fk7rH5kF58` to verify the implementation of the `build_vocabulary` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5151eb55",
        "outputId": "ec0b1857-171f-4c56-ae41-df8ac6ba9fcc"
      },
      "source": [
        "slm.test_build_vocabulary(build_vocabulary)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe88e8f"
      },
      "source": [
        "**Reasoning**:\n",
        "The `build_vocabulary` function has been defined and tested. Now, as per the instructions, I need to run cell `t-mULo_viTXK` to use this function to create the `vocabulary` list, calculate its size, and print the total number of unique tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f22da8d5",
        "outputId": "b8d1a320-f75f-404a-dcf9-b92b1d4faec1"
      },
      "source": [
        "vocabulary = build_vocabulary(tokens)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(\n",
        "    \"Total number of unique tokens in the Africa Galore dataset:\"\n",
        "    f\" {vocabulary_size:,}\"\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique tokens in the Africa Galore dataset: 5,260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f74ccf4e"
      },
      "source": [
        "## Complete Coding Activity 3: Build `index_to_token`\n",
        "\n",
        "### Subtask:\n",
        "Complete the `index_to_token` dictionary in cell `rXlsJqD1VWpK` by mapping each numerical index back to its corresponding token. After implementing the dictionary, run the cell and then run the test cell (`CvATRQpdVWpL`) to verify your implementation.\n",
        "\n",
        "#### Instructions\n",
        "1. In the code cell labeled `rXlsJqD1VWpK`, add code to populate the `index_to_token` dictionary. You can iterate through `token_to_index.items()` to get key-value pairs.\n",
        "2. Run the code cell `rXlsJqD1VWpK` to execute your implementation.\n",
        "3. Run the test cell `CvATRQpdVWpL` to verify your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf801eb"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions in the previous markdown block, the first step is to populate the `index_to_token` dictionary in cell `rXlsJqD1VWpK`. I will iterate through the `token_to_index` dictionary's items to create the reverse mapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8959b29"
      },
      "source": [
        "for token, index in token_to_index.items():\n",
        "    index_to_token[index] = token"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f46fe4d2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block populated the `index_to_token` dictionary. Now, following the instructions, I need to run the test cell `CvATRQpdVWpL` to verify its correctness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "6102de95",
        "outputId": "d7e17896-22ab-4f6f-94b2-6bdd181cc239"
      },
      "source": [
        "slm.test_index_to_token(index_to_token, vocabulary)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>‚ùå Sorry, your answer is not correct.</h3><p>Your index-to-token mapping does not match the expected solution.</p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like a hint? Type Yes or No. yes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Hint:</h3><p>\n",
              "    1. Create a dictionary where the key is <code>index</code> and the value\n",
              "       is <code>token</code>.<br>\n",
              "    2. You can use the Python <code>enumerate</code> function to loop through\n",
              "       the vocabulary and get both the index and the token at the same time.\n",
              "    </p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc87c8f"
      },
      "source": [
        "### Fix: Populate `token_to_index`\n",
        "\n",
        "The `index_to_token` dictionary depends on `token_to_index` being correctly populated. It appears that the cell `mP40_BT2VWpK`, which builds the `token_to_index` dictionary, was not executed.\n",
        "\n",
        "Before proceeding with `Coding Activity 3`, please run the code cell `mP40_BT2VWpK` to create the `token_to_index` mapping.\n",
        "\n",
        "#### Instructions\n",
        "1. Run the code cell `mP40_BT2VWpK` to build the `token_to_index` dictionary.\n",
        "2. Once `token_to_index` is populated, re-run the code cell `e8959b29` (the one that populates `index_to_token`) and then the test cell `6102de95`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf646c19"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test failed because `token_to_index` was empty. Following the instruction in the markdown block, I will first run the code cell `mP40_BT2VWpK` to populate the `token_to_index` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "610aa588"
      },
      "source": [
        "# Build the `token_to_index` dictionary.\n",
        "token_to_index = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "    token_to_index[token] = index"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7fba2d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test failed because `token_to_index` was empty. Following the instruction in the markdown block, I will first run the code cell `mP40_BT2VWpK` to populate the `token_to_index` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c983317a"
      },
      "source": [
        "# Build the `token_to_index` dictionary.\n",
        "token_to_index = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "    token_to_index[token] = index"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601b4c0f"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions from the fix, the `token_to_index` dictionary has now been populated. The next step is to re-run the code cell `e8959b29` which populates the `index_to_token` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9729af87"
      },
      "source": [
        "for token, index in token_to_index.items():\n",
        "    index_to_token[index] = token"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ece0d0"
      },
      "source": [
        "**Reasoning**:\n",
        "The `index_to_token` dictionary has been repopulated after ensuring `token_to_index` was correctly built. Now, as per the instructions in the fix, I need to re-run the test cell `6102de95` to verify the correctness of the `index_to_token` mapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401b0a13",
        "outputId": "01821ce5-ae09-4ed4-ede4-c9f2e899fa13"
      },
      "source": [
        "slm.test_index_to_token(index_to_token, vocabulary)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "908089b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The tokenization and mapping steps are now complete. The next part of the main task is to calculate and visualize the frequencies of the top 20 most frequent tokens. I will start by importing `collections` to use `Counter` for frequency calculation and `matplotlib.pyplot` for plotting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d67b9b0",
        "outputId": "ff1f1baa-7349-4b18-a193-6865d1d5f003"
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Imported collections and matplotlib.pyplot.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported collections and matplotlib.pyplot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476bbb6d"
      },
      "source": [
        "## Analyze and Visualize Token Frequencies\n",
        "\n",
        "### Subtask:\n",
        "Calculate the frequency of each token, identify the top 20, and visualize them in a bar chart.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f73f060e"
      },
      "source": [
        "**Reasoning**:\n",
        "I will calculate the frequency of each token, identify the top 20, extract them into lists, and then create and display a bar chart using matplotlib to visualize these frequencies as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "ca382064",
        "outputId": "c3f4b941-c7e3-4500-fe7d-48616f26f961"
      },
      "source": [
        "token_frequencies = collections.Counter(tokens)\n",
        "top_20_tokens = token_frequencies.most_common(20)\n",
        "\n",
        "tokens_to_plot = [item[0] for item in top_20_tokens]\n",
        "frequencies_to_plot = [item[1] for item in top_20_tokens]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(tokens_to_plot, frequencies_to_plot, color='skyblue')\n",
        "plt.xlabel('Token')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Tokens')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated bar chart for the top 20 most frequent tokens.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated bar chart for the top 20 most frequent tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b6151c"
      },
      "source": [
        "## Summary of Tokenization and Frequency Analysis\n",
        "\n",
        "### Tokenization Steps:\n",
        "1. **Loading and Tokenizing the Dataset**: The `Africa Galore` dataset was loaded, and individual paragraphs were tokenized into a list of words using a `space_tokenize` function.\n",
        "2. **Building the Vocabulary**: A unique set of tokens (the vocabulary) was created from the complete list of tokens. This step ensures that each distinct word or symbol is represented only once.\n",
        "3. **Creating Token-to-Index Mappings**: Two dictionaries, `token_to_index` and `index_to_token`, were built to facilitate conversion between text tokens and their numerical IDs, and vice-versa. These mappings are crucial for preparing text data for machine learning models.\n",
        "4. **Encapsulation in `SimpleWordTokenizer`**: All these steps were then encapsulated within a `SimpleWordTokenizer` class, providing a reusable and organized way to perform tokenization, encoding, and decoding.\n",
        "\n",
        "### Insights from Frequency Analysis:\n",
        "- The bar chart clearly shows the most common words in the `Africa Galore` dataset. As expected in natural language, articles and prepositions such as 'a', 'and', 'the', 'of', 'is', 'with', 'in', 'to', 'for', 'or', 'was', 'as', 'are', 'from', 'by', 'it', 'its', 'their', 'on' dominate the top 20 most frequent tokens.\n",
        "- Capitalized versions of common words (e.g., 'The') also appear due to the simple space tokenization, which does not normalize case. This highlights that basic tokenization might retain case sensitivity, which could be important or require further pre-processing depending on the model's requirements.\n",
        "- The presence of common function words at the top suggests that further text cleaning, such as stop-word removal, might be beneficial for certain natural language processing tasks where the focus is on content-bearing words rather than grammatical connectors.\n",
        "- The high frequency of these words underscores their importance in the overall structure and flow of the text, even if they don't carry significant semantic meaning on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3764cecf"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The Africa Galore dataset contains a total of 19,065 tokens after space tokenization.\n",
        "*   The vocabulary, consisting of unique tokens, comprises 5,260 distinct entries.\n",
        "*   The top 20 most frequent tokens in the dataset are predominantly common English articles, prepositions, and conjunctions (e.g., 'the', 'of', 'and', 'to', 'in', 'a', 'is', 'with', 'for'), indicating typical natural language distribution.\n",
        "*   The tokenization process retained case sensitivity, as evidenced by the presence of both lowercase and capitalized versions of words (e.g., 'the' and 'The') among the tokens.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   For many natural language processing tasks, it would be beneficial to normalize token case (e.g., convert all tokens to lowercase) to reduce vocabulary size and treat 'The' and 'the' as the same word.\n",
        "*   Given the high frequency of common function words, a subsequent step could involve stop-word removal to focus analysis on more semantically rich content words, depending on the specific NLP task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61c107b7"
      },
      "source": [
        "# Task\n",
        "Calculate token lengths for all tokens in the `tokens` list, then analyze their distribution by calculating descriptive statistics (min, max, mean, median), and finally visualize this distribution using a histogram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c24f428"
      },
      "source": [
        "## Calculate Token Lengths\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `tokens` list and calculate the length of each token, storing these lengths in a new list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e672248d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will add a code cell to create an empty list `token_lengths`, iterate through the `tokens` list, calculate the length of each token, append it to `token_lengths`, and then print the first 10 elements and the total count as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a459da",
        "outputId": "32037eb3-c0f1-4115-d3ac-c4727a5919a1"
      },
      "source": [
        "token_lengths = []\n",
        "\n",
        "for token in tokens:\n",
        "    token_lengths.append(len(token))\n",
        "\n",
        "print(f\"First 10 token lengths: {token_lengths[:10]}\")\n",
        "print(f\"Total number of token lengths: {len(token_lengths):,}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 token lengths: [3, 5, 3, 3, 5, 4, 9, 3, 3, 6]\n",
            "Total number of token lengths: 19,065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c149ea54"
      },
      "source": [
        "## Analyze Token Length Distribution\n",
        "\n",
        "### Subtask:\n",
        "Calculate descriptive statistics (minimum, maximum, mean, median) for the `token_lengths` list, and then visualize the distribution using a histogram.\n",
        "\n",
        "#### Instructions\n",
        "1. Import the `numpy` library if not already imported, as it's useful for statistical calculations.\n",
        "2. Calculate the minimum, maximum, mean, and median of the `token_lengths` list.\n",
        "3. Print these descriptive statistics.\n",
        "4. Create a histogram of `token_lengths` using `matplotlib.pyplot`.\n",
        "5. Ensure the histogram has appropriate labels for the x-axis ('Token Length'), y-axis ('Frequency'), and a title ('Distribution of Token Lengths').\n",
        "6. Adjust the number of bins as needed for better visualization of the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff35a79"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions in the markdown block, I will calculate the descriptive statistics for `token_lengths` using `numpy` and then plot a histogram of the distribution using `matplotlib.pyplot`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "2a73d984",
        "outputId": "2b272947-4afe-4cc7-b59c-76788b89baea"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "min_len = np.min(token_lengths)\n",
        "max_len = np.max(token_lengths)\n",
        "mean_len = np.mean(token_lengths)\n",
        "median_len = np.median(token_lengths)\n",
        "\n",
        "print(f\"Minimum token length: {min_len}\")\n",
        "print(f\"Maximum token length: {max_len}\")\n",
        "print(f\"Mean token length: {mean_len:.2f}\")\n",
        "print(f\"Median token length: {median_len}\")\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(token_lengths, bins=range(1, max_len + 2), edgecolor='black')\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Token Lengths')\n",
        "plt.xticks(np.arange(0, max_len + 1, 2))\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Descriptive statistics calculated and histogram plotted for token lengths.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum token length: 0\n",
            "Maximum token length: 22\n",
            "Mean token length: 5.17\n",
            "Median token length: 5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive statistics calculated and histogram plotted for token lengths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71e13c4a"
      },
      "source": [
        "## Summary of Token Length Analysis\n",
        "\n",
        "### Descriptive Statistics:\n",
        "- **Minimum token length**: The shortest token in the dataset has a length of `0` characters. This often indicates empty strings or artifacts from tokenization (e.g., splitting on multiple spaces resulting in an empty string token).\n",
        "- **Maximum token length**: The longest token observed is `22` characters.\n",
        "- **Mean token length**: The average token length across the dataset is approximately `5.17` characters.\n",
        "- **Median token length**: The median token length is `5.0` characters, suggesting that half of the tokens have 5 or fewer characters.\n",
        "\n",
        "### Insights from Histogram:\n",
        "- The histogram shows a distribution of token lengths, with a clear peak for tokens around 3-7 characters. This is a common pattern in natural languages, where many words fall within this length range.\n",
        "- There is a noticeable frequency of tokens with length 0, which needs to be considered for further cleaning if empty tokens are not desired for the specific NLP task.\n",
        "- The distribution has a long tail, indicating that while most tokens are short, there are some significantly longer tokens present in the dataset.\n",
        "\n",
        "### Further Considerations:\n",
        "- Tokens of length 0 (empty strings) are usually removed during data cleaning unless they serve a specific purpose.\n",
        "- Depending on the NLP task, very short or very long tokens might be treated differently (e.g., filtering out single-character tokens or very long tokens that might be malformed words or special characters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e2c850"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A total of 19,065 token lengths were calculated from the `tokens` list. The first 10 token lengths observed were `[3, 5, 3, 3, 5, 4, 9, 3, 3, 6]`.\n",
        "*   The descriptive statistics for token lengths are:\n",
        "    *   **Minimum token length**: 0 characters.\n",
        "    *   **Maximum token length**: 22 characters.\n",
        "    *   **Mean token length**: Approximately 5.17 characters.\n",
        "    *   **Median token length**: 5.0 characters.\n",
        "*   The histogram of token lengths shows a clear peak in frequency for tokens between 3 and 7 characters, which is typical for natural languages.\n",
        "*   There is a noticeable frequency of tokens with a length of 0, indicating the presence of empty strings in the dataset.\n",
        "*   The distribution exhibits a long tail, meaning that while most tokens are short, a smaller number of significantly longer tokens are also present.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Address Empty Tokens**: Investigate the source of 0-length tokens. Depending on the NLP task, these are typically artifacts of tokenization that should be removed during data cleaning unless they serve a specific purpose.\n",
        "*   **Contextual Token Handling**: Based on the specific NLP task (e.g., text classification, sentiment analysis), consider how very short (e.g., 1-2 characters) or very long tokens should be handled. They might be filtered, stemmed, or lemmatized differently to optimize model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620b4038"
      },
      "source": [
        "# Task\n",
        "The task is complete. I have provided a comprehensive summary of all analyses performed, including tokenization steps, token frequency insights, and token length distribution observations, along with potential next steps and further considerations for working with this dataset, in the last markdown cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38d1d43d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a comprehensive summary of all analyses performed, including tokenization steps, token frequency insights, and token length distribution observations. Discuss potential next steps or further considerations for working with this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a643c0d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "- A comprehensive summary of all performed analyses was successfully generated.\n",
        "- The summary included a detailed explanation of the tokenization steps applied to the data.\n",
        "- Insights derived from the analysis of token frequencies were presented.\n",
        "- Observations regarding the distribution of token lengths were also provided within the summary.\n",
        "\n",
        "### Insights or Next Steps\n",
        "- The generated summary also outlined potential next steps and considerations for future work with the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42121e0b"
      },
      "source": [
        "# Task\n",
        "**User provided response:**\n",
        "\n",
        "The task is complete. I have provided a comprehensive summary of all analyses performed, including tokenization steps, token frequency insights, and token length distribution observations, along with potential next steps and further considerations for working with this dataset, in the last markdown cell.\n",
        "\n",
        "## Introduce Project Visualizations\n",
        "\n",
        "### Subtask:\n",
        "Add a new text cell to introduce the upcoming flowcharts and diagrams. Explain their purpose in clarifying the data preparation workflow for your 'Africa Galore' project and enhancing its presentation.\n",
        "\n",
        "**Reasoning**:\n",
        "I will create a new text cell as instructed to introduce the visualization section. This cell will set the context for the upcoming diagrams, explaining their role in illustrating the data preparation workflow for the 'Africa Galore' project and how they contribute to a clearer and more professional presentation.\n",
        "\n",
        "```python\n",
        "# Create a new text cell.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144e0ca8"
      },
      "source": [
        "## Introduce Project Visualizations\n",
        "\n",
        "### Subtask:\n",
        "Add a new text cell to introduce the upcoming flowcharts and diagrams. Explain their purpose in clarifying the data preparation workflow for your 'Africa Galore' project and enhancing its presentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c9ab5c7"
      },
      "source": [
        "## Introduce Project Visualizations\n",
        "\n",
        "### Subtask:\n",
        "Add a new text cell to introduce the upcoming flowcharts and diagrams. Explain their purpose in clarifying the data preparation workflow for your 'Africa Galore' project and enhancing its presentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a27a126"
      },
      "source": [
        "This section introduces the visualizations that will be used to illustrate the data preparation workflow for the 'Africa Galore' project. The upcoming flowcharts and diagrams are designed to clarify each step of the process, from raw data ingestion to the final formatted dataset, thereby enhancing the understanding and presentation of our methodology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b4ac52"
      },
      "source": [
        "## Visualize Overall Data Preparation Workflow\n",
        "\n",
        "### Subtask:\n",
        "Create a high-level conceptual flowchart or block diagram outlining the entire data preparation pipeline, from 'Raw Text Dataset' to 'Numerical Encoding for SLM'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcbb44a"
      },
      "source": [
        "## Overall Data Preparation Workflow for SLMs\n",
        "\n",
        "The data preparation pipeline transforms raw text into a numerical format suitable for training Small Language Models (SLMs). This process involves several sequential stages:\n",
        "\n",
        "1.  **Raw Text Dataset**: This is the initial input stage, consisting of unstructured human-readable text data (e.g., the Africa Galore dataset paragraphs). This is the starting point for all subsequent processing.\n",
        "\n",
        "2.  **Tokenization**: In this stage, the raw text is broken down into smaller, meaningful units called \"tokens.\" For this lab, a simple space tokenizer is used to split text into words or punctuation marks. This converts a continuous string into a list of discrete tokens.\n",
        "\n",
        "3.  **Building Vocabulary**: After tokenization, a vocabulary is constructed. This involves collecting all unique tokens from the entire dataset. The vocabulary serves as the definitive set of all words and symbols that the SLM will recognize.\n",
        "\n",
        "4.  **Creating Mappings (token_to_index, index_to_token)**: This crucial stage establishes a bidirectional mapping between each unique token in the vocabulary and a unique numerical identifier (ID). `token_to_index` maps a token (string) to its corresponding integer ID, while `index_to_token` maps an integer ID back to its token (string). These mappings are essential for the model to process and generate numerical data.\n",
        "\n",
        "5.  **Numerical Encoding for SLM**: Finally, the tokenized text is converted into sequences of numerical IDs using the `token_to_index` mapping. Each token in a text sequence is replaced by its assigned numerical ID. This step produces the fully numerical input that the SLM requires for training and inference, as neural networks operate on numerical representations rather than raw text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deac1a55"
      },
      "source": [
        "## Overall Data Preparation Workflow for SLMs\n",
        "\n",
        "The data preparation pipeline transforms raw text into a numerical format suitable for training Small Language Models (SLMs). This process involves several sequential stages:\n",
        "\n",
        "1.  **Raw Text Dataset**: This is the initial input stage, consisting of unstructured human-readable text data (e.g., the Africa Galore dataset paragraphs). This is the starting point for all subsequent processing.\n",
        "\n",
        "2.  **Tokenization**: In this stage, the raw text is broken down into smaller, meaningful units called \"tokens.\" For this lab, a simple space tokenizer is used to split text into words or punctuation marks. This converts a continuous string into a list of discrete tokens.\n",
        "\n",
        "3.  **Building Vocabulary**: After tokenization, a vocabulary is constructed. This involves collecting all unique tokens from the entire dataset. The vocabulary serves as the definitive set of all words and symbols that the SLM will recognize.\n",
        "\n",
        "4.  **Creating Mappings (token_to_index, index_to_token)**: This crucial stage establishes a bidirectional mapping between each unique token in the vocabulary and a unique numerical identifier (ID). `token_to_index` maps a token (string) to its corresponding integer ID, while `index_to_token` maps an integer ID back to its token (string). These mappings are essential for the model to process and generate numerical data.\n",
        "\n",
        "5.  **Numerical Encoding for SLM**: Finally, the tokenized text is converted into sequences of numerical IDs using the `token_to_index` mapping. Each token in a text sequence is replaced by its assigned numerical ID. This step produces the fully numerical input that the SLM requires for training and inference, as neural networks operate on numerical representations rather than raw text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c408c0e0"
      },
      "source": [
        "## Visualize Overall Data Preparation Workflow\n",
        "\n",
        "### Subtask:\n",
        "Create a high-level conceptual flowchart or block diagram outlining the entire data preparation pipeline, from 'Raw Text Dataset' to 'Numerical Encoding for SLM'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40076bdc"
      },
      "source": [
        "```markdown\n",
        "## Visualize Overall Data Preparation Workflow\n",
        "\n",
        "### Subtask:\n",
        "Create a high-level conceptual flowchart or block diagram outlining the entire data preparation pipeline, from 'Raw Text Dataset' to 'Numerical Encoding for SLM'.\n",
        "\n",
        "#### Instructions\n",
        "1. Create a new markdown cell.\n",
        "2. Use markdown formatting (e.g., bullet points, indentation, arrows like `-->` or `->`) to construct a text-based flowchart that conceptually represents the data preparation pipeline.\n",
        "3. The flowchart should clearly show the main stages: 'Raw Text Dataset' --> 'Tokenization' --> 'Building Vocabulary' --> 'Creating Mappings (token_to_index, index_to_token)' --> 'Numerical Encoding for SLM'.\n",
        "4. Ensure the visualization is clear and easy to follow, reflecting the sequential nature of the process.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ac24cd"
      },
      "source": [
        "```markdown\n",
        "### Data Preparation Workflow Flowchart\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Raw Text Dataset] --> B{Tokenization}\n",
        "    B --> C{Building Vocabulary}\n",
        "    C --> D{Creating Mappings (token_to_index, index_to_token)}\n",
        "    D --> E[Numerical Encoding for SLM]\n",
        "```\n",
        "\n",
        "**Explanation of Workflow:**\n",
        "\n",
        "1.  **Raw Text Dataset**: The initial input consists of unprocessed text data.\n",
        "2.  **Tokenization**: This stage breaks down the raw text into smaller units called tokens (e.g., words, subwords, characters) using a tokenizer (e.g., `space_tokenize`).\n",
        "3.  **Building Vocabulary**: From the collected tokens, a unique set of tokens, known as the vocabulary, is constructed. This vocabulary serves as the dictionary of all recognized tokens.\n",
        "4.  **Creating Mappings (token_to_index, index_to_token)**: Two essential mappings are created:\n",
        "    *   `token_to_index`: A dictionary that maps each unique token in the vocabulary to a unique numerical ID.\n",
        "    *   `index_to_token`: A dictionary that provides the reverse mapping, converting numerical IDs back to their corresponding tokens.\n",
        "5.  **Numerical Encoding for SLM**: Finally, the text data is transformed into a sequence of numerical IDs using the `token_to_index` mapping. This numerical representation is the format required for training Small Language Models (SLMs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5a316"
      },
      "source": [
        "### Data Preparation Workflow Flowchart\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Raw Text Dataset] --> B{Tokenization}\n",
        "    B --> C{Building Vocabulary}\n",
        "    C --> D{Creating Mappings (token_to_index, index_to_token)}\n",
        "    D --> E[Numerical Encoding for SLM]\n",
        "```\n",
        "\n",
        "**Explanation of Workflow:**\n",
        "\n",
        "1.  **Raw Text Dataset**: The initial input consists of unprocessed text data.\n",
        "2.  **Tokenization**: This stage breaks down the raw text into smaller units called tokens (e.g., words, subwords, characters) using a tokenizer (e.g., `space_tokenize`).\n",
        "3.  **Building Vocabulary**: From the collected tokens, a unique set of tokens, known as the vocabulary, is constructed. This vocabulary serves as the dictionary of all recognized tokens.\n",
        "4.  **Creating Mappings (token_to_index, index_to_token)**: Two essential mappings are created:\n",
        "    *   `token_to_index`: A dictionary that maps each unique token in the vocabulary to a unique numerical ID.\n",
        "    *   `index_to_token`: A dictionary that provides the reverse mapping, converting numerical IDs back to their corresponding tokens.\n",
        "5.  **Numerical Encoding for SLM**: Finally, the text data is transformed into a sequence of numerical IDs using the `token_to_index` mapping. This numerical representation is the format required for training Small Language Models (SLMs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da650f28"
      },
      "source": [
        "## Illustrate Tokenization and Indexing Detail\n",
        "\n",
        "### Subtask:\n",
        "Develop a more detailed conceptual diagram focusing on the specific steps of tokenization and numerical indexing for a sample sentence, showing tokenization, lookup in `token_to_index`, and the resulting numerical IDs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3c55bbe"
      },
      "source": [
        "## Illustrate Tokenization and Indexing Detail\n",
        "\n",
        "### Subtask:\n",
        "Develop a more detailed conceptual diagram focusing on the specific steps of tokenization and numerical indexing for a sample sentence, showing tokenization, lookup in `token_to_index`, and the resulting numerical IDs.\n",
        "\n",
        "#### Conceptual Diagram: Tokenization and Numerical Indexing\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Sample Sentence: \"The quick brown fox.\"] --> B{space_tokenize()}\n",
        "    B --> C[Tokens: [\"The\", \"quick\", \"brown\", \"fox.\"]]\n",
        "    C --> D{token_to_index Lookup}\n",
        "    D --> E[Numerical IDs: [token_id(\"The\"), token_id(\"quick\"), token_id(\"brown\"), token_id(\"fox.\")]]\n",
        "```\n",
        "\n",
        "#### Detailed Explanation:\n",
        "\n",
        "This diagram illustrates the process of converting a human-readable sentence into a sequence of numerical IDs, which is a crucial step before feeding text data into a transformer model.\n",
        "\n",
        "1.  **Sample Sentence Input**: The process begins with a raw text string, for example, `\"The quick brown fox.\"` This is the input that needs to be prepared for the model.\n",
        "\n",
        "2.  **Tokenization (`space_tokenize()`)**: The first step is to break down the sentence into smaller, manageable units called **tokens**. Our `space_tokenize()` function (which uses `re.split(\" +\", text)`) performs this by splitting the sentence wherever one or more spaces occur. For our sample sentence, this results in the list of tokens: `[\"The\", \"quick\", \"brown\", \"fox.\"]`.\n",
        "\n",
        "3.  **`token_to_index` Lookup**: Each token then needs to be converted into a unique numerical identifier. This is achieved by consulting the `token_to_index` dictionary. This dictionary acts as a lookup table, where each unique token from our entire dataset's vocabulary is mapped to a specific integer ID. For each token in our tokenized list, we find its corresponding numerical ID in `token_to_index`.\n",
        "\n",
        "4.  **Numerical IDs Output**: The final output is a sequence (a list) of these numerical IDs. For instance, if `\"The\"` maps to `10`, `\"quick\"` to `25`, `\"brown\"` to `42`, and `\"fox.\" `to `58`, the output would be `[10, 25, 42, 58]`. This sequence of integers is the format that transformer models can process to learn patterns and generate text."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "W9eSg4k_QE9E",
        "kWRihzWaQGnE",
        "9gQPoK4TkH1q",
        "_dyB98kFkTF9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}